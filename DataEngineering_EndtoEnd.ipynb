{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI5V7qpdbkJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c7de56-5d77-42b1-f136-819b8c13ff84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Dependencies"
      ],
      "metadata": {
        "id": "C0DlohJrRW0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.3.2 pandas pyarrow fastparquet requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqwSm5ZORZzo",
        "outputId": "6faf64cf-e932-46c7-fd6c-6dd3706d4d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.3.2\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2024.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting py4j==0.10.9.5 (from pyspark==3.3.2)\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.11.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from fastparquet) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastparquet-2024.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824009 sha256=89fbacea5451c01d6e78fcb93ce896b13a473240aa2f29337eb4196bdcc9b44e\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/99/74/4ba9a39bf51affed2cbf91a45688ea2a125775239338534f85\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark, fastparquet\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.1\n",
            "    Uninstalling pyspark-3.5.1:\n",
            "      Successfully uninstalled pyspark-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.3 requires pyspark[connect]~=3.5.1, but you have pyspark 3.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fastparquet-2024.11.0 py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSV'S Landing"
      ],
      "metadata": {
        "id": "1udRR_5BSLU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from datetime import date\n",
        "\n",
        "#project root in Google Drive\n",
        "PROJECT = \"/content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project\"\n",
        "\n",
        "RAW = PROJECT + \"/data/raw\"\n",
        "LANDING_ROOT = PROJECT + \"/landing\"\n",
        "today = date.today().isoformat()\n",
        "\n",
        "# Ensure root folders exist\n",
        "os.makedirs(RAW, exist_ok=True)\n",
        "os.makedirs(LANDING_ROOT, exist_ok=True)\n",
        "\n",
        "# Convert CSV → Parquet and land into date-partitioned folders\n",
        "for fname in sorted(os.listdir(RAW)):\n",
        "    if fname.lower().endswith(\".csv\"):\n",
        "        path = os.path.join(RAW, fname)\n",
        "        df = pd.read_csv(path)\n",
        "\n",
        "        table_name = os.path.splitext(fname)[0]\n",
        "        outdir = os.path.join(LANDING_ROOT, table_name, today)\n",
        "        os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "        outpath = os.path.join(outdir, \"data.parquet\")\n",
        "        df.to_parquet(outpath, index=False)\n",
        "\n",
        "        print(\"Saved\", outpath, \"rows:\", len(df))\n",
        "\n"
      ],
      "metadata": {
        "id": "Gnd79G_KSP-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "API's Landing\n"
      ],
      "metadata": {
        "id": "tdsRamIvU22Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, requests, pandas as pd\n",
        "from datetime import date\n",
        "\n",
        "# --- Correct Base URL (must end with /) ---\n",
        "API_BASE = \"https://demodata.grapecity.com/\"\n",
        "\n",
        "# --- Your project path in Drive ---\n",
        "PROJECT = \"/content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project\"\n",
        "\n",
        "LANDING_ROOT = PROJECT + \"/landing\"\n",
        "today = date.today().isoformat()\n",
        "\n",
        "# Endpoints (your list is correct)\n",
        "endpoints = {\n",
        "    \"businessEntityAddresses\": \"adventureworks/api/v1/businessEntityAddresses\",\n",
        "    \"addressTypes\": \"adventureworks/api/v1/addressTypes\",\n",
        "    \"billOfMaterials\": \"adventureworks/api/v1/billOfMaterials\",\n",
        "    \"productCategories\": \"adventureworks/api/v1/productCategories\",\n",
        "    \"productModels\": \"adventureworks/api/v1/productModels\",\n",
        "    \"products\": \"adventureworks/api/v1/products\",\n",
        "    \"productSubcategories\": \"adventureworks/api/v1/productSubcategories\",\n",
        "    \"productDescriptions\": \"adventureworks/api/v1/productDescriptions\",\n",
        "    \"productReviews\": \"adventureworks/api/v1/productReviews\",\n",
        "    \"salesOrders\": \"adventureworks/api/v1/salesOrders\",\n",
        "    \"salesReasons\": \"adventureworks/api/v1/salesReasons\",\n",
        "    \"salesTaxRates\": \"adventureworks/api/v1/salesTaxRates\",\n",
        "    \"creditCards\": \"adventureworks/api/v1/creditCards\",\n",
        "    \"customers\": \"adventureworks/api/v1/customers\",\n",
        "    \"employees\": \"adventureworks/api/v1/employees\",\n",
        "    \"locations\": \"adventureworks/api/v1/locations\",\n",
        "\n",
        "}\n",
        "\n",
        "os.makedirs(LANDING_ROOT, exist_ok=True)\n",
        "\n",
        "for name, path in endpoints.items():\n",
        "    url = API_BASE + path\n",
        "    print(\"GET\", url)\n",
        "\n",
        "    r = requests.get(url, timeout=60)\n",
        "    r.raise_for_status()\n",
        "\n",
        "    payload = r.json()\n",
        "\n",
        "    if isinstance(payload, dict):\n",
        "        arr = None\n",
        "        for v in payload.values():\n",
        "            if isinstance(v, list):\n",
        "                arr = v\n",
        "                break\n",
        "        if arr is None:\n",
        "            arr = payload if isinstance(payload, list) else []\n",
        "    else:\n",
        "        arr = payload\n",
        "\n",
        "    if not arr:\n",
        "        print(\"No data for\", name)\n",
        "        continue\n",
        "\n",
        "    df = pd.DataFrame(arr)\n",
        "\n",
        "    outdir = os.path.join(LANDING_ROOT, f\"api_{name}\", today)\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "    outpath = os.path.join(outdir, \"data.parquet\")\n",
        "    df.to_parquet(outpath, index=False)\n",
        "\n",
        "    print(\"Saved\", outpath, \"rows:\", len(df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NewC8vJU6Wn",
        "outputId": "ff4e5a3e-f915-4509-b8ea-3e24203c5df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GET https://demodata.grapecity.com/adventureworks/api/v1/businessEntityAddresses\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_businessEntityAddresses/2025-11-30/data.parquet rows: 100\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/addressTypes\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_addressTypes/2025-11-30/data.parquet rows: 6\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/billOfMaterials\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_billOfMaterials/2025-11-30/data.parquet rows: 100\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/productCategories\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_productCategories/2025-11-30/data.parquet rows: 4\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/productModels\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_productModels/2025-11-30/data.parquet rows: 128\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/products\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_products/2025-11-30/data.parquet rows: 100\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/productSubcategories\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_productSubcategories/2025-11-30/data.parquet rows: 37\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/productDescriptions\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_productDescriptions/2025-11-30/data.parquet rows: 762\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/productReviews\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_productReviews/2025-11-30/data.parquet rows: 4\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/salesOrders\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_salesOrders/2025-11-30/data.parquet rows: 100\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/salesReasons\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_salesReasons/2025-11-30/data.parquet rows: 10\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/salesTaxRates\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_salesTaxRates/2025-11-30/data.parquet rows: 29\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/creditCards\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_creditCards/2025-11-30/data.parquet rows: 100\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/customers\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_customers/2025-11-30/data.parquet rows: 100\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/employees\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_employees/2025-11-30/data.parquet rows: 100\n",
            "GET https://demodata.grapecity.com/adventureworks/api/v1/locations\n",
            "Saved /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing/api_locations/2025-11-30/data.parquet rows: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Landing Bronze Parquet"
      ],
      "metadata": {
        "id": "WW2k7xrcjsVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import current_timestamp, to_date\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"AdventureWorks_Bronze\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "LANDING_ROOT = PROJECT + \"/landing\"\n",
        "BRONZE_ROOT = PROJECT + \"/bronze\"\n",
        "today = date.today().isoformat()\n"
      ],
      "metadata": {
        "id": "jsCMahn1jyl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Products Bronze Transform"
      ],
      "metadata": {
        "id": "lfnfQZDEkATV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read landing parquet for products (if CSV-to-parquet saved it)\n",
        "p_path = f\"{LANDING_ROOT}/products/{today}/data.parquet\"\n",
        "# if API used, path may be landing/api_products/...\n",
        "if not os.path.exists(p_path):\n",
        "    p_path = f\"{LANDING_ROOT}/api_products/{today}/data.parquet\"\n",
        "\n",
        "df = spark.read.parquet(p_path)\n",
        "# Example schema enforcement: cast columns if required (customize per table)\n",
        "# For demonstration, just deduplicate and add ingestion columns:\n",
        "df_clean = df.dropDuplicates().withColumn(\"load_timestamp\", current_timestamp()).withColumn(\"ingestion_date\", to_date(current_timestamp()))\n",
        "\n",
        "out_dir = f\"{BRONZE_ROOT}/products/{today}\"\n",
        "df_clean.write.mode(\"overwrite\").parquet(out_dir)\n",
        "print(\"Bronze products written to\", out_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCW-hZ9zkDl0",
        "outputId": "f9ab2735-08bf-4276-94d8-9c3c2a95308f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bronze products written to /content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/bronze/products/2025-11-30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Data Parquets for Bronze Layer\n"
      ],
      "metadata": {
        "id": "1AZfRujFs2vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "src = \"/content/drive/MyDrive/AdventureWorks_DE_Project/dataengneering-project/landing\"\n",
        "dst = \"/content/bronze.zip\"\n",
        "\n",
        "shutil.make_archive(\"/content/bronze\", 'zip', src)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/bronze.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "a4vfU5CQsmpO",
        "outputId": "f0ee9137-113a-4770-816a-38205a6db3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7066316d-12d2-40f1-b215-021d8affac4b\", \"bronze.zip\", 116542)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "files = glob.glob(r\"C:\\Users\\SARA\\Downloads\\dataengineering-project\\bronze\\**\\*.parquet\", recursive=True)\n",
        "print(files)\n",
        "print(\"Total:\", len(files))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qETbVMd57wEW",
        "outputId": "97787af8-ad6e-4666-84ec-1a838ae5d2b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "Total: 0\n"
          ]
        }
      ]
    }
  ]
}