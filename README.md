# AdventureWorks — End-to-End Data Platform (Medallion Architecture)

**Author:** Sara Ali  
**Project:** Data Engineering End-to-End (AdventureWorks dataset)  
**Date:** (12th December, 2025)

---

## Project Summary (one-paragraph)
This project builds a local, end-to-end data engineering pipeline for AdventureWorks using a medallion architecture (Landing → Bronze → Silver → Gold). Data is ingested from CSVs, a Swagger REST API, and SQL dumps into MinIO (landing). Processing is performed with Python (Pandas + PySpark) to create Bronze tables in PostgreSQL, then SCD Type 2 logic transforms Bronze → Silver (dimensions & facts). Gold layer contains business-ready aggregated tables and views for BI and visualization. Orchestration is implemented with Prefect flows (Airflow alternative). Visualizations are produced with Python (Matplotlib / Plotly).

---

## Repo structure
├── README.md
├── .gitignore
├── docker-compose.yml # (Optional) Compose to run MinIO, Postgres, Jupyter, (Airflow)
├── configs/
│ └── config.json # base API URL, MinIO credentials
├── scripts/
│ ├── extract_csv_to_minio.py
│ ├── extract_api_to_minio.py
│ ├── load_parquets_to_postgres.py
│ ├── bronze_to_silver.py
│ └── silver_sales_fact_orders.py
├── scd2/
│ ├── scd2_loader.py
│ ├── scd2_customers.py
│ ├── scd2_employees.py
│ └── scd2_products.py
├── spark/ # Optional PySpark scripts
│ └── transform_bronze_spark.py
├── sql/
│ ├── schema_bronze.sql
│ ├── schema_silver.sql
│ ├── schema_gold.sql
│ └── sample_queries.sql
├── gold_layer/
│ ├── gold_utils.py
│ ├── gold_monthly_summary.py
│ ├── gold_sales_by_category.py
│ ├── gold_sales_by_region.py
│ ├── gold_hr_headcount.py
│ ├── gold_product_performance.py
│ ├── gold_customer_lifetime.py
│ └── run_all_gold.py
├── orchestration/
│ ├── prefect_utils.py
│ ├── sales_pipeline.py
│ ├── hr_pipeline.py
│ ├── production_pipeline.py
│ ├── customer_pipeline.py
│ └── run_all_flows.py
├── airflow/ # Keep DAGs here so teacher sees them
│ └── dags/
│ ├── sales_dag.py
│ ├── hr_dag.py
│ └── customer_dag.py
├── notebooks/
│ ├── exploration.ipynb
│ └── visualization.ipynb
├── outputs/
│ └── charts/ # PNGs generated by visualize scripts
└── docs/
└── architecture.png # exported diagram from AI tool


---

## How to run the project (development / local)
> These commands assume PostgreSQL and MinIO are running, or you are using local CSV + Colab for processing.

### 1) Start services (optional, if using docker-compose)
```bash
docker compose up -d
# Airflow UI: http://localhost:8080 (if running)
# MinIO: http://localhost:9001
# Jupyter: http://localhost:8888

2)Prepare Config
{
  "api_base_url": "https://demodata.grapecity.com/swagger/index.html?urls.primaryName=Restful%20AdventureWorks",
  "minio": {"endpoint":"localhost:9000","access_key":"minio","secret_key":"minio123","bucket":"adventureworks"}
}

3)Ingest Data(Landing Layer)
CSVs: python scripts/extract_csv_to_minio.py

API: python scripts/extract_api_to_minio.py

(If you don't have MinIO, scripts can write to the data/landing/ folder locally.)

4) Load landing → bronze (parquet → Postgres)
python scripts/load_parquets_to_postgres.py
# This reads parquet files from landing, writes bronze_ tables to Postgres

5) Bronze → Silver (SCD2 dimensions)

Run SCD2 scripts:

python scd2/scd2_customers.py
python scd2/scd2_employees.py
python scd2/scd2_products.py
# ... repeat for all dimensions

6) Build Silver Fact table
python scripts/silver_sales_fact_orders.py

7) Build Gold tables & views

You can run files individually in gold_layer/ or run all:

python gold_layer/run_all_gold.py

8) Visualize

Run the visualization script (creates outputs/charts/*.png):

python visualize_gold.py
# or open notebooks/visualization.ipynb in Jupyter / Colab

## Orchestration — Prefect (Airflow alternative)

Because Airflow couldn't be run on the local machine, this project uses **Prefect** to automate and schedule pipelines. Prefect flows are implemented per department and mirror the original Airflow DAG flow:

`extract_data → load_to_minio → load_to_bronze → transform_to_silver → build_gold_views → notify_success`

Files implementing Prefect flows:
orchestration/prefect_utils.py # reusable Prefect tasks
orchestration/sales_pipeline.py # Sales DAG (Prefect flow)
orchestration/hr_pipeline.py # HR DAG (Prefect flow)
orchestration/production_pipeline.py # Production DAG (Prefect flow)
orchestration/customer_pipeline.py # Customer DAG (Prefect flow)
orchestration/finance_pipeline.py # Finance DAG (optional)
orchestration/run_all_flows.py # Master runner

How to run Prefect flows locally (no server required):
- Run one pipeline:

python orchestration/sales_pipeline.py
Run all pipelines:

python orchestration/run_all_flows.py


Optional: if you want dashboard / scheduling and your machine has space, you can install Prefect and run the UI:

pip install prefect
prefect orion start
# Then open: http://127.0.0.1:4200 
